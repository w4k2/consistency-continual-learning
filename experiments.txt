ER, SplitCifar100, lr = 0.001, neopochs = 50, acc = 0.28220000000000006 <- it was run with the return task id = True, but for some reason when return_task_id=False, the accuracy drops to 0.07???
CR, SplitCifar100, lr = 0.001, neopochs = 50, L1 reg, acc = 0.2894
CR, SplitCifar100, lr = 0.001, neopochs = 50, L2 reg, acc = 0.2993
CR, SplitCifar100, lr = 0.001, neopochs = 50, Linf reg, acc = 0.2967
Linf, Cifar10 transform stats, acc = 0.298


new runs
ER SplitCifar100 lr = 0.001, nepochs=50, acc = 0.28040000000000004
CR SplitCifar100 lr = 0.03, nepochs=50, acc =  0.27380000000000004
CR repeat the same parameters, seed = 100, acc =  0.2653
CR alpha and beta switched, first conv kernel size = 3, acc =  0.2913
CR MSE 0.3187
CR Linf acc =  0.33

CR 20 tasks, epochs=50, lr=0.003, seed=42, Linf, acc =  0.3366
same, epochs=10 acc =  0.30
same epochs=30, acc =  0.30439999999999995
5 tasks lr=0.003 acc =  0.2263
for comparison no task id, 10 epochs: acc = 0.115 why??


baseline:
python main.py --method="consistency" --lr=0.003 --seed=42 --n_experiences=20 --regularisation_type="Linf" --n_epochs=50
acc =  0.337
python main.py --method="consistency" --lr=0.003 --seed=42 --n_experiences=20 --regularisation_type="Linf" --n_epochs=50 --beta=0.8 --alpha=0.5
acc =  0.33609999999999995
python main.py --method="consistency" --lr=0.003 --seed=42 --n_experiences=20 --regularisation_type="Linf" --n_epochs=50 --beta=0.5 --alpha=0.7
acc =  0.3267

python main.py --method="extended_consistency" --lr=0.003 --seed=42 --n_experiences=20 --regularisation_type="Linf" --n_epochs=50 --last_k_layers=2 &> extended_last_2_layers.txt
acc =  0.34130000000000005
python main.py --method="extended_consistency" --lr=0.003 --seed=42 --n_experiences=20 --regularisation_type="Linf" --n_epochs=50 --last_k_layers=3 --device="cuda:1" &> extended_last_3_layers.txt
acc =  0.3399
python main.py --method="extended_consistency" --lr=0.003 --seed=42 --n_experiences=20 --regularisation_type="Linf" --n_epochs=50 --last_k_layers=4
acc = 0.3278
python main.py --method="extended_consistency" --lr=0.003 --seed=42 --n_experiences=20 --regularisation_type="Linf" --n_epochs=50 --last_k_layers=6


original repo CR [Class-IL]: 23.06 %
original repo CIFAR100 5 tasks class-incremental acc = 40.58 %
original repo resnet18 with conv1 kernel size 7 acc = 31.2 %
original repo class-il L1 seed=42 25.31 %
same but seed=43 24.98
same but seed=44 acc 24.2
same but seed=10 21.78
pretex task linf seed 10 41.42
pretex task linf seed 42 44.02


experiments with various sizes of training data:
original teacher model acc afeter training with 50 epochs: 0.72688
dataset size = 0.5 acc = 0.64266

distilation:
training with distilation 50 epochs, T = 5, alpha = 0.8, acc = 0.696
same 100 epochs acc = 0.7536
training with distilation 100 epochs, T = 20, alpha = 0.8, acc = 0.75436
same, but dataset size = 0.5 acc = 0.6744
same, but dataset size = 0.25 acc = 0.51362
same, but dataset size = 0.1 acc = 0.33452


Compare Linf, KD, and L2 normalization cosine similarity
python main.py --method="extended_consistency" --lr=0.003 --seed=42 --n_experiences=20 --regularisation_type="Linf" --n_epochs=50
acc = 0.3370000000000001
python main.py --method="extended_consistency" --lr=0.003 --seed=42 --n_experiences=20 --regularisation_type="KD" --n_epochs=50
acc = 
python main.py --method="extended_consistency" --lr=0.003 --seed=42 --n_experiences=20 --regularisation_type="cosine" --n_epochs=50
acc = 